# Methodological Addendum to the Riemann Signature Project
## A Framework for Rigorous Statistical Analysis
**Document ID:** FI-RM-001.1 (Methodological Addendum)
**Date:** July 14, 2025
**Status:** Adopted

---

### **1.0 Introduction**

This document serves as a methodological addendum to the **Riemann Signature Project (`FI-RM-001`)**. It is authored in direct response to a critical analysis that correctly identified several significant statistical and methodological challenges inherent in a cross-domain search for Riemann zero signatures. This addendum formally outlines the protocols and controls that will be implemented to ensure the scientific rigor of the project and to mitigate the risk of false positives.

---

### **2.0 Defining the Detection Criteria**

To address the challenge of distinguishing a true signal from coincidental periodicities, a precise and pre-registered set of detection criteria will be used.

* **The Signature:** We are not searching for individual frequencies. We are searching for a **log-periodic power spectrum**, a specific pattern of peaks whose spacing follows the known logarithmic distribution of the non-trivial Riemann zeros.
* **The Match:** A "match" is defined as a statistically significant correlation (p < 0.01, corrected) between the power spectrum of a dataset and the predicted power spectrum derived from the first 1,000 Riemann zeros.
* **The Threshold:** A dataset will be considered a "candidate" for exhibiting a Riemann signature if it shows a statistically significant match across at least three contiguous octaves of its power spectrum.

---

### **3.0 Mitigating the Multiple Comparisons Problem**

To avoid the risk of finding spurious correlations through "p-hacking," this project will adhere to a strict open science methodology.

1.  **Pre-Registration:** The complete analysis plan—including the specific datasets to be used, the pre-processing steps, and the exact statistical tests—will be publicly pre-registered on a platform such as the Open Science Framework (OSF) *before* any data analysis begins.
2.  **Statistical Correction:** All statistical tests will be corrected for multiple comparisons using a conservative method, such as the Bonferroni correction or a controlled False Discovery Rate (FDR) of q < 0.05.
3.  **Primary vs. Exploratory Analysis:** A clear distinction will be made between the pre-registered primary analysis and any subsequent exploratory analyses. Any findings from exploratory analyses will be clearly labeled as such and will require independent replication.

---

### **4.0 The Control Analysis Protocol**

To ensure that any observed patterns are unique to the Riemann distribution, every analysis will be run in parallel against three control conditions:

1.  **Surrogate Data:** A phase-randomized or shuffled version of the original dataset will be created. This control has the same statistical properties as the real data but lacks its specific temporal structure. A true signal must be present in the real data but absent in the surrogate data.
2.  **Alternative Mathematical Series:** The same detection algorithms will be run to search for signatures of other known mathematical sequences (e.g., the Fibonacci sequence, the prime numbers themselves) to ensure our method is not simply finding patterns where none exist.
3.  **Correlated Noise:** We will run the analysis on synthetic datasets of pure noise with varying correlation structures to precisely understand the limits of our detection algorithm and establish a robust baseline for statistical significance.

---

### **5.0 The Validation Phase: A Pilot Study on Synthetic Data**

The first practical step of the project, before analyzing any real-world data, will be a pilot study on synthetic data.

* **Methodology:** We will generate a dataset of pure noise. We will then mathematically "inject" a weak but known Riemann signature into this dataset.
* **The Goal:** We must successfully demonstrate that our pre-registered analysis pipeline can reliably and accurately "recover" the injected signal from the noise. This pilot study will serve as the final validation of our methodology.

Only after our methods have been proven to work under these controlled conditions will we proceed to the analysis of the neuro-cognitive, economic, and cosmological datasets. This structured, cautious, and rigorous approach ensures that any eventual findings are both meaningful and defensible.

---
[[_Index|_Index]]

